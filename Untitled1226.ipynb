{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40d0668-d95e-4464-a5c9-53bae08f55c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Images: 8728 | Classes: 437\n",
      "[INFO] Train: 7633 | Val: 1095\n",
      "Epoch 1/3\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 2s/step - acc: 0.0117 - loss: 6.0230 - val_acc: 0.0119 - val_loss: 6.0649\n",
      "Epoch 2/3\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 2s/step - acc: 0.0538 - loss: 5.8971 - val_acc: 0.0091 - val_loss: 6.1290\n",
      "Epoch 3/3\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 2s/step - acc: 0.0788 - loss: 5.4494 - val_acc: 9.1324e-04 - val_loss: 6.3437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Val Top-1 Acc: 0.0119 | Top-5 Acc: 0.0320\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\model.h5\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\preprocessor.pkl\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\model_config.yaml\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\metrics.json\n",
      "\n",
      "[INFO] Done. If you still see only one class, your paths likely point to a container folder.\n",
      "Try setting DATA_DIR to the level where subfolders are actual individual IDs, or provide a labels CSV.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# WildTrack — SeaTurtleID2022 Re-ID baseline (robust)\n",
    "# Fixes: single-class issue, top-5 crash, smarter label parsing\n",
    "# Saves: model.h5, preprocessor.pkl, model_config.yaml, metrics.json\n",
    "# ============================================\n",
    "import os, json, pickle, random\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import yaml\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "DATA_DIR   = r\"C:\\Users\\sagni\\Downloads\\WildTrack\\archive\\turtles-data\\data\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\sagni\\Downloads\\WildTrack\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "IMG_SIZE     = (224, 224)\n",
    "BATCH        = 32\n",
    "EPOCHS       = 3\n",
    "VAL_SPLIT    = 0.15\n",
    "MAX_CLASSES  = None           # keep all if available\n",
    "MIN_IMAGES_PER_CLASS = 2      # lower to avoid dropping many IDs\n",
    "EMBED_DIM    = 256\n",
    "BACKBONE     = \"EfficientNetB0\"  # or \"MobileNetV2\"\n",
    "\n",
    "# -------------------------\n",
    "# Utils\n",
    "# -------------------------\n",
    "def find_images(root, exts=(\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\")):\n",
    "    out = []\n",
    "    for dp, _, files in os.walk(root):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(exts):\n",
    "                out.append(os.path.join(dp, f))\n",
    "    return out\n",
    "\n",
    "def try_labels_csv(root):\n",
    "    # Try to auto-detect a CSV with columns mapping image->id\n",
    "    candidate_csvs = [\n",
    "        \"labels.csv\",\"train.csv\",\"metadata.csv\",\"ids.csv\",\"annotations.csv\",\n",
    "        \"train_labels.csv\",\"train_annotations.csv\"\n",
    "    ]\n",
    "    for name in candidate_csvs:\n",
    "        p = os.path.join(root, name)\n",
    "        if os.path.isfile(p):\n",
    "            try:\n",
    "                df = pd.read_csv(p)\n",
    "                cl = [c.lower() for c in df.columns]\n",
    "                img_col = None\n",
    "                for c in [\"image\",\"img\",\"path\",\"file\",\"filename\",\"filepath\"]:\n",
    "                    if c in cl: img_col = df.columns[cl.index(c)]; break\n",
    "                id_col = None\n",
    "                for c in [\"id\",\"individual_id\",\"label\",\"class\",\"identity\"]:\n",
    "                    if c in cl: id_col = df.columns[cl.index(c)]; break\n",
    "                if img_col and id_col:\n",
    "                    m = df[[img_col,id_col]].rename(columns={img_col:\"image\", id_col:\"id\"})\n",
    "                    return m\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "# Smarter label-from-path: skip container names\n",
    "SKIP_DIRS = set([\n",
    "    \"data\",\"dataset\",\"datasets\",\"images\",\"imgs\",\"img\",\n",
    "    \"train\",\"val\",\"valid\",\"validation\",\"test\",\"all\",\"photos\",\"pictures\"\n",
    "])\n",
    "def smart_label_from_path(p):\n",
    "    parts = os.path.normpath(p).split(os.sep)\n",
    "    # Start from parent and walk up until a non-container folder is found\n",
    "    for i in range(len(parts)-2, -1, -1):\n",
    "        name = parts[i]\n",
    "        if name.lower() not in SKIP_DIRS:\n",
    "            return name\n",
    "    # Fallback to immediate parent\n",
    "    return os.path.basename(os.path.dirname(p))\n",
    "\n",
    "# -------------------------\n",
    "# Build (path, label) table\n",
    "# -------------------------\n",
    "images, labels = [], []\n",
    "\n",
    "# Prefer a CSV mapping if present\n",
    "df_map = try_labels_csv(DATA_DIR)\n",
    "if df_map is not None:\n",
    "    for _, r in df_map.iterrows():\n",
    "        p = r[\"image\"]\n",
    "        if not os.path.isabs(p):\n",
    "            p = os.path.join(DATA_DIR, p)\n",
    "        if os.path.isfile(p):\n",
    "            images.append(p)\n",
    "            labels.append(str(r[\"id\"]))\n",
    "else:\n",
    "    # Fallback: discover all images and derive labels from path\n",
    "    all_imgs = find_images(DATA_DIR)\n",
    "    if not all_imgs:\n",
    "        raise RuntimeError(f\"No images found under {DATA_DIR}. Check the path.\")\n",
    "    for p in all_imgs:\n",
    "        labels.append(smart_label_from_path(p))\n",
    "        images.append(p)\n",
    "\n",
    "df = pd.DataFrame({\"path\": images, \"label\": labels})\n",
    "\n",
    "# Drop tiny classes\n",
    "vc = df[\"label\"].value_counts()\n",
    "keep = vc[vc >= MIN_IMAGES_PER_CLASS].index\n",
    "df = df[df[\"label\"].isin(keep)].reset_index(drop=True)\n",
    "\n",
    "# Optional cap\n",
    "if MAX_CLASSES is not None and df[\"label\"].nunique() > MAX_CLASSES:\n",
    "    top_ids = df[\"label\"].value_counts().head(MAX_CLASSES).index\n",
    "    df = df[df[\"label\"].isin(top_ids)].reset_index(drop=True)\n",
    "\n",
    "classes = sorted(df[\"label\"].unique().tolist())\n",
    "n_classes = len(classes)\n",
    "print(f\"[INFO] Images: {len(df)} | Classes: {n_classes}\")\n",
    "if n_classes < 2:\n",
    "    print(\"[WARN] Only one class detected. Training a classifier is not meaningful. \"\n",
    "          \"Consider pointing DATA_DIR to a deeper folder (where subfolders are individual IDs), \"\n",
    "          \"or include/enable a labels CSV. Proceeding anyway and skipping Top-5 metrics.\")\n",
    "\n",
    "# Stratified train/val\n",
    "rng = np.random.RandomState(SEED)\n",
    "df[\"rnd\"] = rng.rand(len(df))\n",
    "val_mask = df.groupby(\"label\")[\"rnd\"].transform(lambda s: s.rank(pct=True)) <= VAL_SPLIT\n",
    "df_train = df[~val_mask].drop(columns=[\"rnd\"]).reset_index(drop=True)\n",
    "df_val   = df[val_mask].drop(columns=[\"rnd\"]).reset_index(drop=True)\n",
    "print(f\"[INFO] Train: {len(df_train)} | Val: {len(df_val)}\")\n",
    "\n",
    "label2id = {c:i for i,c in enumerate(classes)}\n",
    "id2label = {i:c for c,i in label2id.items()}\n",
    "\n",
    "# -------------------------\n",
    "# tf.data pipelines\n",
    "# -------------------------\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def decode_img(path):\n",
    "    x = tf.io.read_file(path)\n",
    "    x = tf.image.decode_image(x, channels=3, expand_animations=False)\n",
    "    x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "    x = tf.image.resize(x, IMG_SIZE)\n",
    "    return x\n",
    "\n",
    "def py_map(path, label_str):\n",
    "    idx = label2id[label_str.numpy().decode(\"utf-8\")]\n",
    "    return path, idx\n",
    "\n",
    "def tf_map(path, label_str):\n",
    "    p, y = tf.py_function(py_map, [path, label_str], [tf.string, tf.int64])\n",
    "    p.set_shape([]); y.set_shape([])\n",
    "    img = decode_img(p)\n",
    "    return img, tf.cast(y, tf.int32)\n",
    "\n",
    "def augment(img, y):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_brightness(img, 0.1)\n",
    "    img = tf.image.random_contrast(img, 0.9, 1.1)\n",
    "    return img, y\n",
    "\n",
    "def make_ds(frame, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((frame[\"path\"].values, frame[\"label\"].values))\n",
    "    if training: ds = ds.shuffle(len(frame), seed=SEED)\n",
    "    ds = ds.map(tf_map, num_parallel_calls=AUTOTUNE)\n",
    "    if training: ds = ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_ds(df_train, True)\n",
    "val_ds   = make_ds(df_val, False)\n",
    "\n",
    "# -------------------------\n",
    "# Model\n",
    "# -------------------------\n",
    "def build_model(nc:int, embed_dim:int=256, backbone:str=\"EfficientNetB0\"):\n",
    "    inputs = keras.Input(shape=(*IMG_SIZE,3))\n",
    "    if backbone == \"MobileNetV2\":\n",
    "        base = keras.applications.MobileNetV2(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
    "    else:\n",
    "        base = keras.applications.EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
    "    x = layers.GlobalAveragePooling2D()(base.output)\n",
    "    x = layers.Dense(embed_dim, activation=None, name=\"embedding\")(x)\n",
    "    x = layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=-1), name=\"l2norm\")(x)\n",
    "    if nc == 1:\n",
    "        # Binary-ish head (degenerate dataset). Use sigmoid to avoid softmax(1) warning.\n",
    "        logits = layers.Dense(1, activation=\"sigmoid\", name=\"classifier\")(x)\n",
    "        loss   = \"binary_crossentropy\"\n",
    "        metrics= [keras.metrics.BinaryAccuracy(name=\"acc\")]\n",
    "    else:\n",
    "        logits = layers.Dense(nc, activation=\"softmax\", name=\"classifier\")(x)\n",
    "        loss   = \"sparse_categorical_crossentropy\"\n",
    "        metrics= [keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    "    model = keras.Model(inputs, logits, name=\"reid_classifier\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "model = build_model(n_classes, EMBED_DIM, BACKBONE)\n",
    "\n",
    "# Class weights (only if >1 class)\n",
    "class_weight = None\n",
    "if n_classes > 1:\n",
    "    y_train_idx = df_train[\"label\"].map(label2id).values\n",
    "    cw = compute_class_weight(\"balanced\", classes=np.arange(n_classes), y=y_train_idx)\n",
    "    class_weight = {i: float(w) for i,w in enumerate(cw)}\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(os.path.join(OUTPUT_DIR, \"tmp_best.keras\"),\n",
    "                                    monitor=\"val_acc\", mode=\"max\", save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=2, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Train\n",
    "# -------------------------\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Reload best\n",
    "try:\n",
    "    model = keras.models.load_model(os.path.join(OUTPUT_DIR, \"tmp_best.keras\"))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# Evaluate\n",
    "# -------------------------\n",
    "y_true, y_prob = [], []\n",
    "for bx, by in val_ds:\n",
    "    pr = model.predict(bx, verbose=0)\n",
    "    y_prob.append(pr)\n",
    "    y_true.append(by.numpy())\n",
    "y_prob = np.concatenate(y_prob, axis=0)\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "if n_classes == 1:\n",
    "    # binary head: y_prob shape (N,1); fake y_pred as zeros (single class)\n",
    "    y_pred = (y_prob >= 0.5).astype(int).squeeze()\n",
    "    top1 = float(accuracy_score(np.zeros_like(y_true), np.zeros_like(y_pred)))  # degenerate but defined\n",
    "    top5 = None\n",
    "else:\n",
    "    y_pred = y_prob.argmax(axis=1)\n",
    "    top1 = float(accuracy_score(y_true, y_pred))\n",
    "    # Guard top-k when shapes/classes mismatch or nc < 2\n",
    "    try:\n",
    "        top5 = float(top_k_accuracy_score(y_true, y_prob, k=min(5, n_classes), labels=np.arange(n_classes)))\n",
    "    except Exception:\n",
    "        top5 = None\n",
    "\n",
    "print(f\"[INFO] Val Top-1 Acc: {top1:.4f}\" + (\"\" if top5 is None else f\" | Top-5 Acc: {top5:.4f}\"))\n",
    "\n",
    "# -------------------------\n",
    "# Save artifacts\n",
    "# -------------------------\n",
    "# model.h5\n",
    "h5_path = os.path.join(OUTPUT_DIR, \"model.h5\")\n",
    "model.save(h5_path)\n",
    "print(f\"[INFO] Saved -> {h5_path}\")\n",
    "\n",
    "# preprocessor.pkl\n",
    "preproc = {\n",
    "    \"image_size\": IMG_SIZE,\n",
    "    \"embed_dim\": EMBED_DIM,\n",
    "    \"backbone\": BACKBONE,\n",
    "    \"label2id\": label2id,\n",
    "    \"id2label\": {int(k):v for k,v in id2label.items()},\n",
    "    \"class_count\": n_classes,\n",
    "    \"min_images_per_class\": MIN_IMAGES_PER_CLASS,\n",
    "    \"val_split\": VAL_SPLIT,\n",
    "    \"selected_classes\": classes,\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"preprocessor.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(preproc, f)\n",
    "print(f\"[INFO] Saved -> {os.path.join(OUTPUT_DIR, 'preprocessor.pkl')}\")\n",
    "\n",
    "# model_config.yaml\n",
    "cfg = {\n",
    "    \"created\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"task\": \"wildlife_reid_baseline_classifier\",\n",
    "    \"data_dir\": DATA_DIR,\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"image_size\": list(IMG_SIZE),\n",
    "    \"batch\": BATCH,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"backbone\": BACKBONE,\n",
    "    \"embed_dim\": EMBED_DIM,\n",
    "    \"num_classes\": n_classes,\n",
    "    \"training\": {\"optimizer\":\"Adam\",\"lr\":1e-3,\n",
    "                 \"loss\": \"binary_crossentropy\" if n_classes==1 else \"sparse_categorical_crossentropy\",\n",
    "                 \"metrics\": [\"acc\"]}\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"model_config.yaml\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(cfg, f, sort_keys=False)\n",
    "print(f\"[INFO] Saved -> {os.path.join(OUTPUT_DIR, 'model_config.yaml')}\")\n",
    "\n",
    "# metrics.json\n",
    "metrics = {\n",
    "    \"val_top1_accuracy\": top1,\n",
    "    \"val_top5_accuracy\": top5,\n",
    "    \"num_validation_samples\": int(len(df_val)),\n",
    "    \"num_train_samples\": int(len(df_train)),\n",
    "    \"num_classes\": n_classes\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"[INFO] Saved -> {os.path.join(OUTPUT_DIR, 'metrics.json')}\")\n",
    "\n",
    "print(\"\\n[INFO] Done. If you still see only one class, your paths likely point to a container folder.\")\n",
    "print(\"Try setting DATA_DIR to the level where subfolders are actual individual IDs, or provide a labels CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904fd36-f592-4fec-ae0b-4883ae39a636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
