{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0aab71-bb83-4b4a-b2ca-f43ac0c3196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Validation samples: 1095 | Classes in val: 337\n",
      "[WARN] Direct H5 load failed (expected with legacy Lambda): Exception encountered when calling Lambda.call().\n",
      "\n",
      "\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n",
      "\n",
      "Arguments received by Lambda.call():\n",
      "  • args=('<KerasTensor shape=(None, 256), dtype=float32, sparse=False, ragged=False, name=keras_tensor_1007>',)\n",
      "  • kwargs={'mask': 'None'}\n",
      "[INFO] Loaded weights from H5 into rebuilt architecture (by_name, skip_mismatch).\n",
      "[INFO] Validation accuracy (Top-1): 0.0018\n",
      "[INFO] Saved heatmap -> C:\\Users\\sagni\\Downloads\\WildTrack\\confusion_matrix.png\n",
      "[INFO] Saved per-class accuracy graph -> C:\\Users\\sagni\\Downloads\\WildTrack\\accuracy_per_class.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# WildTrack — Accuracy graph + Confusion Matrix heatmap (robust loader)\n",
    "# Fixes AttributeError: 'keras' has no attribute 'saving'\n",
    "# Uses version-safe register_keras_serializable import\n",
    "# ============================================\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    USE_SNS = True\n",
    "except Exception:\n",
    "    USE_SNS = False\n",
    "\n",
    "# -------------------------\n",
    "# Version-safe registration\n",
    "# -------------------------\n",
    "try:\n",
    "    from tensorflow.keras.utils import register_keras_serializable\n",
    "except Exception:\n",
    "    try:\n",
    "        from keras.utils import register_keras_serializable  # pragma: no cover\n",
    "    except Exception:  # very old TF/Keras\n",
    "        def register_keras_serializable(package=\"Custom\", name=None):\n",
    "            def deco(obj): return obj\n",
    "            return deco\n",
    "\n",
    "# -------------------------\n",
    "# Paths (edit if needed)\n",
    "# -------------------------\n",
    "DATA_DIR   = r\"C:\\Users\\sagni\\Downloads\\WildTrack\\archive\\turtles-data\\data\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\sagni\\Downloads\\WildTrack\"\n",
    "MODEL_KERAS = os.path.join(OUTPUT_DIR, \"model.keras\")\n",
    "MODEL_H5    = os.path.join(OUTPUT_DIR, \"model.h5\")\n",
    "PP_PATH     = os.path.join(OUTPUT_DIR, \"preprocessor.pkl\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Load preprocessor\n",
    "# -------------------------\n",
    "with open(PP_PATH, \"rb\") as f:\n",
    "    preproc = pickle.load(f)\n",
    "\n",
    "IMG_SIZE   = tuple(preproc.get(\"image_size\", (224,224)))\n",
    "VAL_SPLIT  = float(preproc.get(\"val_split\", 0.15))\n",
    "label2id   = dict(preproc[\"label2id\"])\n",
    "id2label   = {int(k): v for k, v in preproc[\"id2label\"].items()}\n",
    "SEED       = 42\n",
    "\n",
    "# Optional model config (helps rebuilding when using H5)\n",
    "backbone_name = str(preproc.get(\"backbone\", \"EfficientNetB0\"))\n",
    "embed_dim     = int(preproc.get(\"embed_dim\", 256))\n",
    "use_l2norm    = bool(preproc.get(\"use_l2norm\", True))\n",
    "\n",
    "# -------------------------\n",
    "# Serializable L2 normalize (avoids Lambda pitfalls)\n",
    "# -------------------------\n",
    "@register_keras_serializable(package=\"WildTrack\")\n",
    "class L2Normalize(layers.Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "    def call(self, x):\n",
    "        return tf.math.l2_normalize(x, axis=self.axis)\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"axis\": self.axis})\n",
    "        return cfg\n",
    "\n",
    "def build_classifier(n_classes:int, image_size, backbone=\"EfficientNetB0\",\n",
    "                     embed_dim=256, use_l2=True):\n",
    "    inputs = keras.Input(shape=(image_size[0], image_size[1], 3))\n",
    "    bb = backbone.lower()\n",
    "    if bb == \"efficientnetb0\":\n",
    "        base = keras.applications.EfficientNetB0(include_top=False, weights=None, pooling=\"avg\")\n",
    "    elif bb == \"resnet50\":\n",
    "        base = keras.applications.ResNet50(include_top=False, weights=None, pooling=\"avg\")\n",
    "    elif bb == \"mobilenetv2\":\n",
    "        base = keras.applications.MobileNetV2(include_top=False, weights=None, pooling=\"avg\")\n",
    "    else:\n",
    "        base = keras.applications.EfficientNetB0(include_top=False, weights=None, pooling=\"avg\")\n",
    "    x = base(inputs)\n",
    "    x = layers.Dropout(0.2, name=\"dropout\")(x)\n",
    "    if embed_dim and embed_dim > 0:\n",
    "        x = layers.Dense(embed_dim, name=\"emb\")(x)\n",
    "    if use_l2:\n",
    "        x = L2Normalize(name=\"l2norm\")(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\", name=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"wildtrack_classifier\")\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Robust model loader\n",
    "# -------------------------\n",
    "def load_model_robust(n_classes):\n",
    "    custom = {\"L2Normalize\": L2Normalize}\n",
    "    # 1) Prefer native .keras (no Lambda issues)\n",
    "    if os.path.exists(MODEL_KERAS):\n",
    "        try:\n",
    "            m = keras.models.load_model(MODEL_KERAS, custom_objects=custom)\n",
    "            print(f\"[INFO] Loaded native model: {MODEL_KERAS}\")\n",
    "            return m\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Could not load model.keras:\", e)\n",
    "\n",
    "    # 2) Rebuild arch and load weights from H5 (skip mismatches)\n",
    "    if os.path.exists(MODEL_H5):\n",
    "        try:\n",
    "            # Try direct H5 load first (may fail if it had Lambda)\n",
    "            m = keras.models.load_model(MODEL_H5, compile=False, custom_objects=custom)\n",
    "            print(f\"[INFO] Loaded H5 model directly: {MODEL_H5}\")\n",
    "            return m\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Direct H5 load failed (expected with legacy Lambda):\", e)\n",
    "\n",
    "        model = build_classifier(\n",
    "            n_classes=n_classes,\n",
    "            image_size=IMG_SIZE,\n",
    "            backbone=backbone_name,\n",
    "            embed_dim=embed_dim,\n",
    "            use_l2=use_l2norm\n",
    "        )\n",
    "        try:\n",
    "            model.load_weights(MODEL_H5, by_name=True, skip_mismatch=True)\n",
    "            print(f\"[INFO] Loaded weights from H5 into rebuilt architecture (by_name, skip_mismatch).\")\n",
    "            return model\n",
    "        except Exception as e2:\n",
    "            print(\"[ERROR] Could not load weights from H5:\", e2)\n",
    "            raise\n",
    "\n",
    "    raise FileNotFoundError(\"No model file found. Expected model.keras or model.h5 in OUTPUT_DIR.\")\n",
    "\n",
    "# -------------------------\n",
    "# Data helpers\n",
    "# -------------------------\n",
    "SKIP_DIRS = set([\n",
    "    \"data\",\"dataset\",\"datasets\",\"images\",\"imgs\",\"img\",\n",
    "    \"train\",\"val\",\"valid\",\"validation\",\"test\",\"all\",\"photos\",\"pictures\"\n",
    "])\n",
    "\n",
    "def smart_label_from_path(p):\n",
    "    parts = os.path.normpath(p).split(os.sep)\n",
    "    for i in range(len(parts)-2, -1, -1):\n",
    "        name = parts[i]\n",
    "        if name.lower() not in SKIP_DIRS:\n",
    "            return name\n",
    "    return os.path.basename(os.path.dirname(p))\n",
    "\n",
    "def find_images(root, exts=(\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\")):\n",
    "    out = []\n",
    "    for dp, _, files in os.walk(root):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(exts):\n",
    "                out.append(os.path.join(dp, f))\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Build validation split (deterministic)\n",
    "# -------------------------\n",
    "all_imgs = find_images(DATA_DIR)\n",
    "if not all_imgs:\n",
    "    raise RuntimeError(f\"No images found under {DATA_DIR}\")\n",
    "\n",
    "paths, labels = [], []\n",
    "for p in all_imgs:\n",
    "    lab = smart_label_from_path(p)\n",
    "    if lab in label2id:        # keep only classes seen at train time\n",
    "        paths.append(p)\n",
    "        labels.append(lab)\n",
    "\n",
    "df = pd.DataFrame({\"path\": paths, \"label\": labels})\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"After filtering to known classes, no images remain. Check DATA_DIR and label map.\")\n",
    "\n",
    "rng = np.random.RandomState(SEED)\n",
    "df[\"rnd\"] = rng.rand(len(df))\n",
    "val_mask = df.groupby(\"label\")[\"rnd\"].transform(lambda s: s.rank(pct=True)) <= VAL_SPLIT\n",
    "df_val   = df[val_mask].drop(columns=[\"rnd\"]).reset_index(drop=True)\n",
    "\n",
    "if df_val.empty:\n",
    "    raise RuntimeError(\"Validation split is empty. Increase VAL_SPLIT or verify dataset layout.\")\n",
    "\n",
    "print(f\"[INFO] Validation samples: {len(df_val)} | Classes in val: {df_val['label'].nunique()}\")\n",
    "\n",
    "# -------------------------\n",
    "# tf.data pipeline\n",
    "# -------------------------\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def decode_img(path):\n",
    "    x = tf.io.read_file(path)\n",
    "    x = tf.image.decode_image(x, channels=3, expand_animations=False)\n",
    "    x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "    x = tf.image.resize(x, IMG_SIZE)\n",
    "    return x\n",
    "\n",
    "def tf_map(path, label_str):\n",
    "    # Map label string to index via label2id\n",
    "    def _map(s):\n",
    "        return np.int32(label2id[s.decode(\"utf-8\")])\n",
    "    y = tf.numpy_function(_map, [label_str], tf.int32)\n",
    "    y.set_shape([])  # scalar\n",
    "    img = decode_img(path)\n",
    "    return img, y\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((df_val[\"path\"].values, df_val[\"label\"].values))\n",
    "val_ds = val_ds.map(tf_map, num_parallel_calls=AUTOTUNE).batch(32).prefetch(AUTOTUNE)\n",
    "\n",
    "# -------------------------\n",
    "# Load model (robust)\n",
    "# -------------------------\n",
    "n_classes = len(label2id)\n",
    "model = load_model_robust(n_classes)\n",
    "\n",
    "# -------------------------\n",
    "# Predict & metrics\n",
    "# -------------------------\n",
    "y_true, y_prob = [], []\n",
    "for bx, by in val_ds:\n",
    "    pr = model.predict(bx, verbose=0)\n",
    "    y_prob.append(pr)\n",
    "    y_true.append(by.numpy())\n",
    "y_prob = np.concatenate(y_prob, axis=0)\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "# Handle binary/single-class heads gracefully\n",
    "multi_class = (y_prob.ndim == 2 and y_prob.shape[1] > 1)\n",
    "if multi_class:\n",
    "    y_pred = y_prob.argmax(axis=1)\n",
    "    label_names = [id2label[i] for i in range(n_classes)]\n",
    "else:\n",
    "    # Sigmoid/one-logit case\n",
    "    y_pred = (y_prob.reshape(-1) >= 0.5).astype(int)\n",
    "    # Derive label set actually present\n",
    "    uniq_ids = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    label_names = [id2label.get(int(i), f\"class_{int(i)}\") for i in uniq_ids]\n",
    "\n",
    "acc = float(accuracy_score(y_true, y_pred))\n",
    "print(f\"[INFO] Validation accuracy (Top-1): {acc:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Confusion Matrix Heatmap\n",
    "# -------------------------\n",
    "if multi_class:\n",
    "    label_ids = np.arange(n_classes)\n",
    "else:\n",
    "    label_ids = np.unique(np.concatenate([y_true, y_pred]))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=label_ids)\n",
    "\n",
    "plt.figure(figsize=(max(6, 0.5*len(label_names)), max(5, 0.5*len(label_names))))\n",
    "if USE_SNS:\n",
    "    sns.heatmap(cm, annot=len(label_names) <= 30, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=label_names, yticklabels=label_names, cbar=True)\n",
    "else:\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(label_names)), label_names, rotation=90)\n",
    "    plt.yticks(range(len(label_names)), label_names)\n",
    "    if len(label_names) <= 30:\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, str(cm[i, j]),\n",
    "                         ha=\"center\", va=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "cm_path = os.path.join(OUTPUT_DIR, \"confusion_matrix.png\")\n",
    "plt.savefig(cm_path, dpi=160)\n",
    "plt.close()\n",
    "print(f\"[INFO] Saved heatmap -> {cm_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# Per-class Accuracy Bar Chart\n",
    "# -------------------------\n",
    "if multi_class:\n",
    "    per_class_acc = []\n",
    "    for i in range(n_classes):\n",
    "        idx = (y_true == i)\n",
    "        if idx.sum() == 0:\n",
    "            per_class_acc.append(np.nan)\n",
    "        else:\n",
    "            per_class_acc.append((y_pred[idx] == i).mean())\n",
    "\n",
    "    plt.figure(figsize=(max(8, 0.6*n_classes), 5))\n",
    "    xs = np.arange(n_classes)\n",
    "    vals = np.array(per_class_acc)\n",
    "    plt.bar(xs, np.nan_to_num(vals, nan=0.0))\n",
    "    plt.xticks(xs, [id2label[i] for i in range(n_classes)], rotation=90)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Per-class Accuracy (Validation)\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    acc_path = os.path.join(OUTPUT_DIR, \"accuracy_per_class.png\")\n",
    "    plt.savefig(acc_path, dpi=160)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Saved per-class accuracy graph -> {acc_path}\")\n",
    "else:\n",
    "    print(\"[INFO] Single-class head detected; skipping per-class bar chart.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb4774-e746-4131-ae40-a72e0aef36a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
