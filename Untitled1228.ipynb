{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fe0af2-e7a2-4f7e-a95b-f5d5acdb526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "[WARN] Direct H5 load failed: Exception encountered when calling Lambda.call().\n",
      "\n",
      "\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n",
      "\n",
      "Arguments received by Lambda.call():\n",
      "  • args=('<KerasTensor shape=(None, 256), dtype=float32, sparse=False, ragged=False, name=keras_tensor_502>',)\n",
      "  • kwargs={'mask': 'None'}\n",
      "[INFO] Loaded weights by_name from H5.\n",
      "[INFO] Using 'l2norm' layer output for embeddings.\n",
      "[INFO] Gallery images: 8728 across 437 IDs\n",
      "[INFO] Saved embeddings -> C:\\Users\\sagni\\Downloads\\WildTrack\\embeddings.npy  shape=(8728, 256)\n",
      "[INFO] Saved metadata -> C:\\Users\\sagni\\Downloads\\WildTrack\\meta.csv\n",
      "[WARN] FAISS not available (No module named 'faiss'). Saved NumPy index -> C:\\Users\\sagni\\Downloads\\WildTrack\\index.npz\n",
      "[INFO] Wrote FastAPI app -> C:\\Users\\sagni\\Downloads\\WildTrack\\app.py\n",
      "[INFO] Wrote demo HTML -> C:\\Users\\sagni\\Downloads\\WildTrack\\index.html\n",
      "\n",
      "[INFO] Index build complete.\n",
      "- embeddings.npy, meta.csv\n",
      "- index.faiss (if faiss available) OR index.npz\n",
      "- app.py (FastAPI) and index.html written to OUTPUT_DIR\n",
      "\n",
      "Run the API:\n",
      "  python app.py\n",
      "Then open:\n",
      "  http://127.0.0.1:8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# WildTrack — Build embedding index + search API (SeaTurtleID)\n",
    "# Saves: embeddings.npy, meta.csv, index.faiss/index.npz\n",
    "# Writes: app.py (FastAPI) and index.html demo\n",
    "# ============================================================\n",
    "import os, io, sys, csv, base64, pickle, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from PIL import Image\n",
    "\n",
    "# ---------- Paths (edit if needed) ----------\n",
    "DATA_DIR   = r\"C:\\Users\\sagni\\Downloads\\WildTrack\\archive\\turtles-data\\data\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\sagni\\Downloads\\WildTrack\"\n",
    "\n",
    "MODEL_KERAS = os.path.join(OUTPUT_DIR, \"model.keras\")\n",
    "MODEL_H5    = os.path.join(OUTPUT_DIR, \"model.h5\")\n",
    "PP_PATH     = os.path.join(OUTPUT_DIR, \"preprocessor.pkl\")\n",
    "\n",
    "EMB_NPY     = os.path.join(OUTPUT_DIR, \"embeddings.npy\")\n",
    "META_CSV    = os.path.join(OUTPUT_DIR, \"meta.csv\")\n",
    "FAISS_IDX   = os.path.join(OUTPUT_DIR, \"index.faiss\")\n",
    "NP_IDX      = os.path.join(OUTPUT_DIR, \"index.npz\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Load preprocessor ----------\n",
    "with open(PP_PATH, \"rb\") as f:\n",
    "    preproc = pickle.load(f)\n",
    "\n",
    "IMG_SIZE   = tuple(preproc.get(\"image_size\", (224, 224)))\n",
    "label2id   = {k: int(v) for k, v in preproc[\"label2id\"].items()}\n",
    "id2label   = {int(k): v for k, v in preproc[\"id2label\"].items()}\n",
    "backbone   = str(preproc.get(\"backbone\", \"EfficientNetB0\"))\n",
    "embed_dim  = int(preproc.get(\"embed_dim\", 256))\n",
    "use_l2     = bool(preproc.get(\"use_l2norm\", True))\n",
    "VAL_SPLIT  = float(preproc.get(\"val_split\", 0.15))\n",
    "SEED       = 42\n",
    "\n",
    "# ---------- Version-safe custom layer (for older Keras) ----------\n",
    "try:\n",
    "    from tensorflow.keras.utils import register_keras_serializable\n",
    "except Exception:\n",
    "    try:\n",
    "        from keras.utils import register_keras_serializable\n",
    "    except Exception:\n",
    "        def register_keras_serializable(package=\"WildTrack\"):\n",
    "            def deco(obj): return obj\n",
    "            return deco\n",
    "\n",
    "@register_keras_serializable(package=\"WildTrack\")\n",
    "class L2Normalize(layers.Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "    def call(self, x):\n",
    "        return tf.math.l2_normalize(x, axis=self.axis)\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"axis\": self.axis})\n",
    "        return cfg\n",
    "\n",
    "# ---------- Robust model loader ----------\n",
    "def build_classifier(n_classes:int, image_size, backbone=\"EfficientNetB0\",\n",
    "                     embed_dim=256, use_l2=True):\n",
    "    inputs = keras.Input(shape=(image_size[0], image_size[1], 3))\n",
    "    bb = backbone.lower()\n",
    "    if bb == \"efficientnetb0\":\n",
    "        base = keras.applications.EfficientNetB0(include_top=False, weights=None, pooling=\"avg\")\n",
    "    elif bb == \"resnet50\":\n",
    "        base = keras.applications.ResNet50(include_top=False, weights=None, pooling=\"avg\")\n",
    "    elif bb == \"mobilenetv2\":\n",
    "        base = keras.applications.MobileNetV2(include_top=False, weights=None, pooling=\"avg\")\n",
    "    else:\n",
    "        base = keras.applications.EfficientNetB0(include_top=False, weights=None, pooling=\"avg\")\n",
    "    x = base(inputs)\n",
    "    x = layers.Dropout(0.2, name=\"dropout\")(x)\n",
    "    if embed_dim and embed_dim > 0:\n",
    "        x = layers.Dense(embed_dim, name=\"emb\")(x)\n",
    "    if use_l2:\n",
    "        x = L2Normalize(name=\"l2norm\")(x)\n",
    "    outputs = layers.Dense(len(id2label), activation=\"softmax\", name=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs, name=\"wildtrack_classifier\")\n",
    "\n",
    "def load_classifier():\n",
    "    custom = {\"L2Normalize\": L2Normalize}\n",
    "    if os.path.exists(MODEL_KERAS):\n",
    "        try:\n",
    "            m = keras.models.load_model(MODEL_KERAS, custom_objects=custom)\n",
    "            print(f\"[INFO] Loaded model: {MODEL_KERAS}\")\n",
    "            return m\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] model.keras load failed:\", e)\n",
    "    if os.path.exists(MODEL_H5):\n",
    "        try:\n",
    "            m = keras.models.load_model(MODEL_H5, compile=False, custom_objects=custom)\n",
    "            print(f\"[INFO] Loaded model: {MODEL_H5}\")\n",
    "            return m\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Direct H5 load failed:\", e)\n",
    "            # Rebuild & load weights (skip mismatches)\n",
    "            m = build_classifier(len(id2label), IMG_SIZE, backbone, embed_dim, use_l2)\n",
    "            try:\n",
    "                m.load_weights(MODEL_H5, by_name=True, skip_mismatch=True)\n",
    "                print(\"[INFO] Loaded weights by_name from H5.\")\n",
    "                return m\n",
    "            except Exception as e2:\n",
    "                print(\"[ERROR] Could not load weights:\", e2)\n",
    "                raise\n",
    "    raise FileNotFoundError(\"Missing model file (.keras or .h5) in OUTPUT_DIR.\")\n",
    "\n",
    "model = load_classifier()\n",
    "\n",
    "# ---------- Build embedding model (pre-softmax) ----------\n",
    "def build_embedding_model(classifier: keras.Model):\n",
    "    # Prefer \"l2norm\" layer if present (already L2-normalized)\n",
    "    try:\n",
    "        l = classifier.get_layer(\"l2norm\")\n",
    "        out = l.output\n",
    "        print(\"[INFO] Using 'l2norm' layer output for embeddings.\")\n",
    "        return keras.Model(classifier.inputs, out)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Else take the tensor feeding the final softmax\n",
    "    # Find softmax Dense layer\n",
    "    softmax_layer = None\n",
    "    for l in classifier.layers[::-1]:\n",
    "        if isinstance(l, layers.Dense) and getattr(l, \"activation\", None) == keras.activations.softmax:\n",
    "            softmax_layer = l\n",
    "            break\n",
    "        if l.name.lower() == \"softmax\":\n",
    "            softmax_layer = l\n",
    "            break\n",
    "    if softmax_layer is None:\n",
    "        # fallback to last Dense or GlobalPool\n",
    "        print(\"[WARN] Softmax layer not found; using last layer output.\")\n",
    "        return keras.Model(classifier.inputs, classifier.layers[-1].output)\n",
    "    emb_tensor = softmax_layer.input  # tensor before softmax\n",
    "    print(\"[INFO] Using pre-softmax tensor for embeddings.\")\n",
    "    emb_model = keras.Model(classifier.inputs, emb_tensor)\n",
    "    # normalize at runtime to keep cosine meaningful\n",
    "    inp = classifier.inputs\n",
    "    x = emb_model(inp)\n",
    "    x = tf.math.l2_normalize(x, axis=-1)\n",
    "    return keras.Model(inp, x)\n",
    "\n",
    "emb_model = build_embedding_model(model)\n",
    "\n",
    "# ---------- Image utils ----------\n",
    "def list_images(root, exts=(\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\")):\n",
    "    out = []\n",
    "    for dp,_,files in os.walk(root):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(exts):\n",
    "                out.append(os.path.join(dp, f))\n",
    "    return out\n",
    "\n",
    "SKIP_DIRS = set([\"data\",\"dataset\",\"datasets\",\"images\",\"imgs\",\"img\",\"train\",\n",
    "                 \"val\",\"valid\",\"validation\",\"test\",\"all\",\"photos\",\"pictures\"])\n",
    "\n",
    "def smart_label_from_path(p):\n",
    "    parts = os.path.normpath(p).split(os.sep)\n",
    "    for i in range(len(parts)-2,-1,-1):\n",
    "        name = parts[i]\n",
    "        if name.lower() not in SKIP_DIRS:\n",
    "            return name\n",
    "    return os.path.basename(os.path.dirname(p))\n",
    "\n",
    "def load_and_preprocess(path):\n",
    "    x = tf.io.read_file(path)\n",
    "    x = tf.image.decode_image(x, channels=3, expand_animations=False)\n",
    "    x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "    x = tf.image.resize(x, IMG_SIZE)\n",
    "    return x\n",
    "\n",
    "# ---------- Build gallery embeddings ----------\n",
    "BATCH = 32\n",
    "paths = list_images(DATA_DIR)\n",
    "if not paths:\n",
    "    raise RuntimeError(f\"No images found under {DATA_DIR}\")\n",
    "\n",
    "labels = [smart_label_from_path(p) for p in paths]\n",
    "# filter to known classes\n",
    "keep_mask = [lab in label2id for lab in labels]\n",
    "paths  = [p for p,k in zip(paths, keep_mask) if k]\n",
    "labels = [l for l,k in zip(labels, keep_mask) if k]\n",
    "\n",
    "print(f\"[INFO] Gallery images: {len(paths)} across {len(set(labels))} IDs\")\n",
    "\n",
    "def batched(iterable, n):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i+n]\n",
    "\n",
    "emb_list = []\n",
    "for chunk in batched(paths, BATCH):\n",
    "    batch = tf.stack([load_and_preprocess(p) for p in chunk], axis=0)\n",
    "    embs  = emb_model.predict(batch, verbose=0)\n",
    "    # L2 normalize (in case embedding head didn't do it)\n",
    "    embs  = embs / (np.linalg.norm(embs, axis=1, keepdims=True) + 1e-12)\n",
    "    emb_list.append(embs.astype(np.float32))\n",
    "\n",
    "embeddings = np.vstack(emb_list)\n",
    "assert embeddings.shape[0] == len(paths)\n",
    "np.save(EMB_NPY, embeddings)\n",
    "print(f\"[INFO] Saved embeddings -> {EMB_NPY}  shape={embeddings.shape}\")\n",
    "\n",
    "# ---------- Save metadata ----------\n",
    "with open(META_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"index\",\"path\",\"label\",\"label_id\"])\n",
    "    for i,(p,l) in enumerate(zip(paths, labels)):\n",
    "        w.writerow([i, p, l, label2id[l]])\n",
    "print(f\"[INFO] Saved metadata -> {META_CSV}\")\n",
    "\n",
    "# ---------- Build FAISS (if available) or NumPy index ----------\n",
    "use_faiss = False\n",
    "try:\n",
    "    import faiss  # faiss-cpu\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)  # cosine if vectors are L2-normalized\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, FAISS_IDX)\n",
    "    use_faiss = True\n",
    "    print(f\"[INFO] Saved FAISS index -> {FAISS_IDX}\")\n",
    "except Exception as e:\n",
    "    np.savez(NP_IDX, embeddings=embeddings)\n",
    "    print(f\"[WARN] FAISS not available ({e}). Saved NumPy index -> {NP_IDX}\")\n",
    "\n",
    "# ---------- Search function ----------\n",
    "def search(query_path, top_k=5):\n",
    "    qimg = load_and_preprocess(query_path)[None, ...]\n",
    "    qemb = emb_model.predict(qimg, verbose=0)[0]\n",
    "    qemb = qemb / (np.linalg.norm(qemb) + 1e-12)\n",
    "\n",
    "    if use_faiss:\n",
    "        import faiss\n",
    "        idx = faiss.read_index(FAISS_IDX)\n",
    "        D, I = idx.search(qemb[None, :].astype(np.float32), top_k)\n",
    "        sims = D[0].tolist()\n",
    "        inds = I[0].tolist()\n",
    "    else:\n",
    "        sims = (embeddings @ qemb.astype(np.float32)).tolist()\n",
    "        inds = np.argsort(sims)[::-1][:top_k].tolist()\n",
    "        sims = [sims[i] for i in inds]\n",
    "\n",
    "    results = []\n",
    "    for rnk,(i,s) in enumerate(zip(inds, sims), 1):\n",
    "        lab = labels[i]\n",
    "        results.append({\n",
    "            \"rank\": rnk,\n",
    "            \"index\": int(i),\n",
    "            \"path\": paths[i],\n",
    "            \"label\": lab,\n",
    "            \"label_id\": int(label2id[lab]),\n",
    "            \"similarity\": float(s)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Quick smoke test (optional): uncomment to try one random image\n",
    "# import random; sample = random.choice(paths); print(search(sample, top_k=5))\n",
    "\n",
    "# ---------- Write FastAPI server (app.py) ----------\n",
    "APP_PY = os.path.join(OUTPUT_DIR, \"app.py\")\n",
    "with open(APP_PY, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f'''# FastAPI visual search for WildTrack (SeaTurtleID)\n",
    "import os, io, csv, base64, json\n",
    "import numpy as np\n",
    "from fastapi import FastAPI, UploadFile, File, Form\n",
    "from fastapi.responses import JSONResponse, HTMLResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from PIL import Image\n",
    "import uvicorn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "OUTPUT_DIR = r\"{OUTPUT_DIR}\"\n",
    "DATA_DIR   = r\"{DATA_DIR}\"\n",
    "EMB_NPY    = r\"{EMB_NPY}\"\n",
    "META_CSV   = r\"{META_CSV}\"\n",
    "FAISS_IDX  = r\"{FAISS_IDX}\"\n",
    "NP_IDX     = r\"{NP_IDX}\"\n",
    "MODEL_KERAS = r\"{MODEL_KERAS}\"\n",
    "MODEL_H5    = r\"{MODEL_H5}\"\n",
    "IMG_SIZE    = {list(IMG_SIZE)}\n",
    "\n",
    "# ---- L2Normalize layer (for loading) ----\n",
    "try:\n",
    "    from tensorflow.keras.utils import register_keras_serializable\n",
    "except Exception:\n",
    "    try:\n",
    "        from keras.utils import register_keras_serializable\n",
    "    except Exception:\n",
    "        def register_keras_serializable(package=\"WildTrack\"):\n",
    "            def deco(obj): return obj\n",
    "            return deco\n",
    "\n",
    "@register_keras_serializable(package=\"WildTrack\")\n",
    "class L2Normalize(keras.layers.Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "    def call(self, x):\n",
    "        return tf.math.l2_normalize(x, axis=self.axis)\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({{\"axis\": self.axis}})\n",
    "        return cfg\n",
    "\n",
    "# ---- Load meta and embeddings ----\n",
    "paths, labels = [], []\n",
    "with open(META_CSV, newline=\"\", encoding=\"utf-8\") as fcsv:\n",
    "    r = csv.DictReader(fcsv)\n",
    "    for row in r:\n",
    "        paths.append(row[\"path\"])\n",
    "        labels.append(row[\"label\"])\n",
    "\n",
    "if os.path.exists(EMB_NPY):\n",
    "    embeddings = np.load(EMB_NPY).astype(np.float32)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Missing embeddings.npy. Run the index builder first.\")\n",
    "\n",
    "use_faiss = False\n",
    "try:\n",
    "    import faiss\n",
    "    if os.path.exists(FAISS_IDX):\n",
    "        index = faiss.read_index(FAISS_IDX)\n",
    "        use_faiss = True\n",
    "except Exception:\n",
    "    if os.path.exists(NP_IDX):\n",
    "        npz = np.load(NP_IDX)\n",
    "        # embeddings already loaded\n",
    "\n",
    "# ---- Load classifier & build embedding model ----\n",
    "def load_classifier():\n",
    "    custom = {{\"L2Normalize\": L2Normalize}}\n",
    "    if os.path.exists(MODEL_KERAS):\n",
    "        try:\n",
    "            return keras.models.load_model(MODEL_KERAS, custom_objects=custom)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] model.keras load failed:\", e)\n",
    "    if os.path.exists(MODEL_H5):\n",
    "        try:\n",
    "            return keras.models.load_model(MODEL_H5, compile=False, custom_objects=custom)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] model.h5 load failed:\", e)\n",
    "    raise FileNotFoundError(\"No model file found.\")\n",
    "\n",
    "clf = load_classifier()\n",
    "\n",
    "def build_embedding_model(classifier):\n",
    "    # Prefer l2norm if present\n",
    "    try:\n",
    "        l = classifier.get_layer(\"l2norm\")\n",
    "        return keras.Model(classifier.inputs, l.output)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Pre-softmax fallback\n",
    "    softmax_layer = None\n",
    "    for l in classifier.layers[::-1]:\n",
    "        if isinstance(l, keras.layers.Dense) and getattr(l, \"activation\", None) == keras.activations.softmax:\n",
    "            softmax_layer = l\n",
    "            break\n",
    "        if l.name.lower() == \"softmax\":\n",
    "            softmax_layer = l\n",
    "            break\n",
    "    if softmax_layer is None:\n",
    "        return keras.Model(classifier.inputs, classifier.layers[-1].output)\n",
    "    emb_tensor = softmax_layer.input\n",
    "    inp = classifier.inputs\n",
    "    x = keras.Model(inp, emb_tensor)(inp)\n",
    "    x = tf.math.l2_normalize(x, axis=-1)\n",
    "    return keras.Model(inp, x)\n",
    "\n",
    "emb_model = build_embedding_model(clf)\n",
    "\n",
    "# ---- Helpers ----\n",
    "def load_and_preprocess_bytes(b: bytes):\n",
    "    img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "    img = img.resize(tuple(IMG_SIZE), Image.BILINEAR)\n",
    "    arr = np.array(img).astype(np.float32) / 255.0\n",
    "    return arr\n",
    "\n",
    "def encode_thumb(path, max_side=256):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        scale = max_side / max(w, h)\n",
    "        if scale < 1:\n",
    "            img = img.resize((int(w*scale), int(h*scale)), Image.BILINEAR)\n",
    "        buf = io.BytesIO()\n",
    "        img.save(buf, format=\"JPEG\", quality=85)\n",
    "        return \"data:image/jpeg;base64,\" + base64.b64encode(buf.getvalue()).decode(\"ascii\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---- Search core ----\n",
    "def search_vector(qemb: np.ndarray, top_k=5):\n",
    "    if use_faiss:\n",
    "        import faiss\n",
    "        D, I = index.search(qemb[None, :].astype(np.float32), top_k)\n",
    "        sims = D[0].tolist()\n",
    "        inds = I[0].tolist()\n",
    "    else:\n",
    "        sims_all = embeddings @ qemb.astype(np.float32)\n",
    "        inds = np.argsort(sims_all)[::-1][:top_k].tolist()\n",
    "        sims = [float(sims_all[i]) for i in inds]\n",
    "    out = []\n",
    "    for rnk,(i,s) in enumerate(zip(inds, sims), 1):\n",
    "        out.append({{\n",
    "            \"rank\": rnk,\n",
    "            \"index\": int(i),\n",
    "            \"path\": paths[i],\n",
    "            \"label\": labels[i],\n",
    "            \"similarity\": float(s),\n",
    "            \"thumbnail\": encode_thumb(paths[i])\n",
    "        }})\n",
    "    return out\n",
    "\n",
    "# ---- FastAPI app ----\n",
    "app = FastAPI(title=\"WildTrack Search API\", version=\"1.0\")\n",
    "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    html = (Path(OUTPUT_DIR) / \"index.html\").read_text(encoding=\"utf-8\")\n",
    "    return HTMLResponse(html)\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def do_search(file: UploadFile = File(...), top_k: int = Form(5)):\n",
    "    b = await file.read()\n",
    "    arr = load_and_preprocess_bytes(b)\n",
    "    qemb = emb_model.predict(arr[None, ...], verbose=0)[0]\n",
    "    qemb = qemb / (np.linalg.norm(qemb) + 1e-12)\n",
    "    results = search_vector(qemb, top_k=top_k)\n",
    "    return JSONResponse({{\"results\": results}})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "''')\n",
    "print(f\"[INFO] Wrote FastAPI app -> {APP_PY}\")\n",
    "\n",
    "# ---------- Write tiny HTML client ----------\n",
    "INDEX_HTML = os.path.join(OUTPUT_DIR, \"index.html\")\n",
    "with open(INDEX_HTML, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\"\"<!doctype html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\"/>\n",
    "<title>WildTrack Visual Search</title>\n",
    "<style>\n",
    "body{font-family:system-ui,Arial;margin:20px;max-width:900px}\n",
    "#grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(180px,1fr));gap:12px;margin-top:16px}\n",
    ".card{border:1px solid #ddd;border-radius:10px;padding:8px;box-shadow:0 1px 4px rgba(0,0,0,.05)}\n",
    ".card img{width:100%;border-radius:8px}\n",
    "small{color:#555}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<h2>WildTrack — SeaTurtleID Search</h2>\n",
    "<p>Upload an image to find visually similar individuals.</p>\n",
    "<input type=\"file\" id=\"file\" accept=\"image/*\"/>\n",
    "<input type=\"number\" id=\"k\" min=\"1\" max=\"20\" value=\"5\" style=\"width:72px;margin-left:8px\"/> top-k\n",
    "<button id=\"btn\">Search</button>\n",
    "<div id=\"status\"></div>\n",
    "<div id=\"grid\"></div>\n",
    "<script>\n",
    "const btn = document.getElementById('btn');\n",
    "btn.onclick = async () => {\n",
    "  const file = document.getElementById('file').files[0];\n",
    "  const k = document.getElementById('k').value;\n",
    "  if(!file){ alert('Choose an image first'); return; }\n",
    "  const fd = new FormData();\n",
    "  fd.append('file', file);\n",
    "  fd.append('top_k', k);\n",
    "  document.getElementById('status').innerText = 'Searching...';\n",
    "  const res = await fetch('/search', { method:'POST', body: fd });\n",
    "  const js = await res.json();\n",
    "  document.getElementById('status').innerText = '';\n",
    "  const grid = document.getElementById('grid');\n",
    "  grid.innerHTML = '';\n",
    "  js.results.forEach(r => {\n",
    "    const div = document.createElement('div');\n",
    "    div.className = 'card';\n",
    "    const img = document.createElement('img');\n",
    "    img.src = r.thumbnail || '';\n",
    "    const p = document.createElement('div');\n",
    "    p.innerHTML = `<b>${r.label}</b><br/><small>${r.path}</small><br/><small>sim=${r.similarity.toFixed(3)}</small>`;\n",
    "    div.appendChild(img);\n",
    "    div.appendChild(p);\n",
    "    grid.appendChild(div);\n",
    "  });\n",
    "};\n",
    "</script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\")\n",
    "print(f\"[INFO] Wrote demo HTML -> {INDEX_HTML}\")\n",
    "\n",
    "print(\"\"\"\n",
    "[INFO] Index build complete.\n",
    "- embeddings.npy, meta.csv\n",
    "- index.faiss (if faiss available) OR index.npz\n",
    "- app.py (FastAPI) and index.html written to OUTPUT_DIR\n",
    "\n",
    "Run the API:\n",
    "  python app.py\n",
    "Then open:\n",
    "  http://127.0.0.1:8000\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbca1af-1447-44d9-a1f1-da7e2b1c1b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
