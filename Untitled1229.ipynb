{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e51c630-d16a-4f3a-90e3-ef3ccede4276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "[WARN] Direct H5 load failed: Exception encountered when calling Lambda.call().\n",
      "\n",
      "\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n",
      "\n",
      "Arguments received by Lambda.call():\n",
      "  • args=('<KerasTensor shape=(None, 256), dtype=float32, sparse=False, ragged=False, name=keras_tensor_502>',)\n",
      "  • kwargs={'mask': 'None'}\n",
      "[INFO] Loaded weights by_name from H5.\n",
      "[INFO] Using 'l2norm' output for embeddings.\n",
      "[INFO] Eligible IDs (>=2 imgs): 437\n",
      "[INFO] Queries: 437 | Gallery: 8291\n",
      "[INFO] Embedding gallery...\n",
      "[INFO] Embedding queries...\n",
      "[INFO] Metrics: {'R@1': 0.13272311212814644, 'R@5': 0.2723112128146453, 'R@10': 0.32036613272311215, 'mAP': 0.04022098200458359, 'num_queries': 437, 'num_gallery': 8291, 'num_ids': 437}\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\metrics.json\n",
      "[INFO] Running t-SNE on (2000, 256)\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\tsne.png\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\retrieval_example_320.png\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\retrieval_example_131.png\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\retrieval_example_365.png\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\retrieval_example_276.png\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\retrieval_example_268.png\n",
      "[INFO] Saved -> C:\\Users\\sagni\\Downloads\\WildTrack\\retrieval_example_413.png\n",
      "[WARN] No conv layer found; skipping Grad-CAM.\n",
      "\n",
      "[INFO] Done. Artifacts:\n",
      " - metrics.json\n",
      " - tsne.png\n",
      " - retrieval_example_*.png\n",
      " - gradcam_*.png (if available)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# WildTrack (SeaTurtleID) — Re-ID Evaluation & Visualization\n",
    "# Saves under OUTPUT_DIR:\n",
    "#  - metrics.json\n",
    "#  - tsne.png\n",
    "#  - retrieval_example_*.png\n",
    "#  - gradcam_*.png   (if a conv layer is found)\n",
    "# Requires: model.{keras|h5}, preprocessor.pkl from your training step\n",
    "# ============================================================\n",
    "import os, io, json, random, math, csv, pickle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# ------------ Paths (edit if needed) ------------\n",
    "DATA_DIR   = r\"C:\\Users\\sagni\\Downloads\\WildTrack\\archive\\turtles-data\\data\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\sagni\\Downloads\\WildTrack\"\n",
    "\n",
    "MODEL_KERAS = os.path.join(OUTPUT_DIR, \"model.keras\")\n",
    "MODEL_H5    = os.path.join(OUTPUT_DIR, \"model.h5\")\n",
    "PP_PATH     = os.path.join(OUTPUT_DIR, \"preprocessor.pkl\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------ Load preprocessor ------------\n",
    "with open(PP_PATH, \"rb\") as f:\n",
    "    preproc = pickle.load(f)\n",
    "\n",
    "IMG_SIZE  = tuple(preproc.get(\"image_size\", (224, 224)))\n",
    "label2id  = {k:int(v) for k,v in preproc[\"label2id\"].items()}\n",
    "id2label  = {int(k):v for k,v in preproc[\"id2label\"].items()}\n",
    "backbone  = str(preproc.get(\"backbone\", \"EfficientNetB0\"))\n",
    "embed_dim = int(preproc.get(\"embed_dim\", 256))\n",
    "use_l2    = bool(preproc.get(\"use_l2norm\", True))\n",
    "SEED      = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ------------ Version-safe serializable L2 ------------\n",
    "try:\n",
    "    from tensorflow.keras.utils import register_keras_serializable\n",
    "except Exception:\n",
    "    try:\n",
    "        from keras.utils import register_keras_serializable\n",
    "    except Exception:\n",
    "        def register_keras_serializable(package=\"WildTrack\"):\n",
    "            def deco(obj): return obj\n",
    "            return deco\n",
    "\n",
    "@register_keras_serializable(package=\"WildTrack\")\n",
    "class L2Normalize(layers.Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "    def call(self, x): return tf.math.l2_normalize(x, axis=self.axis)\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config(); cfg.update({\"axis\": self.axis}); return cfg\n",
    "\n",
    "# ------------ Robust model loader ------------\n",
    "def build_classifier(n_classes:int, image_size, backbone=\"EfficientNetB0\",\n",
    "                     embed_dim=256, use_l2=True):\n",
    "    inputs = keras.Input(shape=(image_size[0], image_size[1], 3))\n",
    "    bb = backbone.lower()\n",
    "    if bb == \"efficientnetb0\":\n",
    "        base = keras.applications.EfficientNetB0(include_top=False, weights=None, pooling=\"avg\")\n",
    "    elif bb == \"resnet50\":\n",
    "        base = keras.applications.ResNet50(include_top=False, weights=None, pooling=\"avg\")\n",
    "    elif bb == \"mobilenetv2\":\n",
    "        base = keras.applications.MobileNetV2(include_top=False, weights=None, pooling=\"avg\")\n",
    "    else:\n",
    "        base = keras.applications.EfficientNetB0(include_top=False, weights=None, pooling=\"avg\")\n",
    "    x = base(inputs)\n",
    "    x = layers.Dropout(0.2, name=\"dropout\")(x)\n",
    "    if embed_dim and embed_dim > 0:\n",
    "        x = layers.Dense(embed_dim, name=\"emb\")(x)\n",
    "    if use_l2:\n",
    "        x = L2Normalize(name=\"l2norm\")(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\", name=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs, name=\"wildtrack_classifier\")\n",
    "\n",
    "def load_classifier():\n",
    "    custom = {\"L2Normalize\": L2Normalize}\n",
    "    if os.path.exists(MODEL_KERAS):\n",
    "        try:\n",
    "            m = keras.models.load_model(MODEL_KERAS, custom_objects=custom)\n",
    "            print(f\"[INFO] Loaded model: {MODEL_KERAS}\")\n",
    "            return m\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] model.keras load failed:\", e)\n",
    "    if os.path.exists(MODEL_H5):\n",
    "        try:\n",
    "            m = keras.models.load_model(MODEL_H5, compile=False, custom_objects=custom)\n",
    "            print(f\"[INFO] Loaded model: {MODEL_H5}\")\n",
    "            return m\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Direct H5 load failed:\", e)\n",
    "            # Rebuild & try by_name\n",
    "            m = build_classifier(len(id2label), IMG_SIZE, backbone, embed_dim, use_l2)\n",
    "            try:\n",
    "                m.load_weights(MODEL_H5, by_name=True, skip_mismatch=True)\n",
    "                print(\"[INFO] Loaded weights by_name from H5.\")\n",
    "                return m\n",
    "            except Exception as e2:\n",
    "                print(\"[ERROR] Could not load weights:\", e2)\n",
    "                raise\n",
    "    raise FileNotFoundError(\"Missing model file (.keras/.h5).\")\n",
    "\n",
    "model = load_classifier()\n",
    "\n",
    "# ------------ Embedding model (pre-softmax) ------------\n",
    "def build_embedding_model(classifier: keras.Model):\n",
    "    # if L2 head exists, use it directly\n",
    "    try:\n",
    "        l = classifier.get_layer(\"l2norm\")\n",
    "        print(\"[INFO] Using 'l2norm' output for embeddings.\")\n",
    "        return keras.Model(classifier.inputs, l.output)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # else, take tensor feeding softmax and normalize\n",
    "    softmax_layer = None\n",
    "    for l in classifier.layers[::-1]:\n",
    "        if isinstance(l, layers.Dense) and getattr(l, \"activation\", None) == keras.activations.softmax:\n",
    "            softmax_layer = l; break\n",
    "        if l.name.lower() == \"softmax\":\n",
    "            softmax_layer = l; break\n",
    "    if softmax_layer is None:\n",
    "        print(\"[WARN] Softmax not found; using last layer.\")\n",
    "        return keras.Model(classifier.inputs, classifier.layers[-1].output)\n",
    "    emb_t = softmax_layer.input\n",
    "    x = emb_t / (tf.norm(emb_t, axis=-1, keepdims=True) + 1e-12)\n",
    "    return keras.Model(classifier.inputs, x)\n",
    "\n",
    "emb_model = build_embedding_model(model)\n",
    "\n",
    "# ------------ Dataset utils ------------\n",
    "def list_images(root, exts=(\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\")):\n",
    "    out = []\n",
    "    for dp,_,files in os.walk(root):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(exts):\n",
    "                out.append(os.path.join(dp, f))\n",
    "    return out\n",
    "\n",
    "SKIP_DIRS = {\"data\",\"dataset\",\"datasets\",\"images\",\"imgs\",\"img\",\"train\",\n",
    "             \"val\",\"valid\",\"validation\",\"test\",\"all\",\"photos\",\"pictures\"}\n",
    "\n",
    "def smart_label_from_path(p):\n",
    "    parts = os.path.normpath(p).split(os.sep)\n",
    "    for i in range(len(parts)-2,-1,-1):\n",
    "        name = parts[i]\n",
    "        if name.lower() not in SKIP_DIRS:\n",
    "            return name\n",
    "    return os.path.basename(os.path.dirname(p))\n",
    "\n",
    "def load_and_preprocess(path):\n",
    "    x = tf.io.read_file(path)\n",
    "    x = tf.image.decode_image(x, channels=3, expand_animations=False)\n",
    "    x = tf.image.convert_image_dtype(x, tf.float32)\n",
    "    x = tf.image.resize(x, IMG_SIZE)\n",
    "    return x\n",
    "\n",
    "# ------------ Build per-ID groups & split ------------\n",
    "all_paths = list_images(DATA_DIR)\n",
    "if not all_paths:\n",
    "    raise RuntimeError(f\"No images found under {DATA_DIR}\")\n",
    "\n",
    "all_labels = [smart_label_from_path(p) for p in all_paths]\n",
    "# keep only labels known to training\n",
    "keep = [lab in label2id for lab in all_labels]\n",
    "paths  = [p for p,k in zip(all_paths, keep) if k]\n",
    "labels = [l for l,k in zip(all_labels, keep) if k]\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for p,l in zip(paths, labels):\n",
    "    groups[l].append(p)\n",
    "\n",
    "# Require at least 2 images per ID for re-ID\n",
    "groups = {l:imgs for l,imgs in groups.items() if len(imgs) >= 2}\n",
    "ids = sorted(groups.keys())\n",
    "print(f\"[INFO] Eligible IDs (>=2 imgs): {len(ids)}\")\n",
    "\n",
    "# Split: 1 query per ID, rest gallery\n",
    "queries, q_labels = [], []\n",
    "gallery, g_labels = [], []\n",
    "for l in ids:\n",
    "    imgs = groups[l].copy()\n",
    "    random.shuffle(imgs)\n",
    "    q = imgs.pop()            # one query\n",
    "    queries.append(q); q_labels.append(l)\n",
    "    for g in imgs:            # remaining gallery\n",
    "        gallery.append(g); g_labels.append(l)\n",
    "\n",
    "print(f\"[INFO] Queries: {len(queries)} | Gallery: {len(gallery)}\")\n",
    "\n",
    "# ------------ Embed (batched) ------------\n",
    "def embed_paths(p_list, batch=32):\n",
    "    out = []\n",
    "    for i in range(0, len(p_list), batch):\n",
    "        chunk = p_list[i:i+batch]\n",
    "        batch_img = tf.stack([load_and_preprocess(p) for p in chunk], axis=0)\n",
    "        emb = emb_model.predict(batch_img, verbose=0)\n",
    "        emb = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-12)\n",
    "        out.append(emb.astype(np.float32))\n",
    "    return np.vstack(out)\n",
    "\n",
    "print(\"[INFO] Embedding gallery...\")\n",
    "G = embed_paths(gallery, batch=32)  # (Ng, D)\n",
    "print(\"[INFO] Embedding queries...\")\n",
    "Q = embed_paths(queries, batch=32)  # (Nq, D)\n",
    "\n",
    "# ------------ Similarity & metrics ------------\n",
    "def rank_search(q, G):\n",
    "    sims = G @ q.astype(np.float32)\n",
    "    idxs = np.argsort(sims)[::-1]\n",
    "    return idxs, sims[idxs]\n",
    "\n",
    "def cmc_map(q_labels, g_labels, Q, G, ks=(1,5,10)):\n",
    "    ks = tuple(sorted(ks))\n",
    "    correct_at = np.zeros((len(q_labels), max(ks)), dtype=np.int32)\n",
    "    ap_list = []\n",
    "    for qi in range(len(q_labels)):\n",
    "        idxs, sims = rank_search(Q[qi], G)\n",
    "        rel = np.array([1 if g_labels[j] == q_labels[qi] else 0 for j in idxs], dtype=np.int32)\n",
    "        # CMC\n",
    "        for r in range(min(len(rel), max(ks))):\n",
    "            correct_at[qi, r] = 1 if rel[:r+1].any() else 0\n",
    "        # AP\n",
    "        if rel.sum() == 0:\n",
    "            ap_list.append(0.0)\n",
    "        else:\n",
    "            # classic AP computation\n",
    "            cum_rel = np.cumsum(rel)\n",
    "            precisions = cum_rel / (np.arange(len(rel)) + 1)\n",
    "            ap = (precisions * rel).sum() / rel.sum()\n",
    "            ap_list.append(float(ap))\n",
    "    recalls = {f\"R@{k}\": float(correct_at[:, k-1].mean()) for k in ks}\n",
    "    metrics = {\n",
    "        **recalls,\n",
    "        \"mAP\": float(np.mean(ap_list)),\n",
    "        \"num_queries\": int(len(q_labels)),\n",
    "        \"num_gallery\": int(len(g_labels)),\n",
    "        \"num_ids\": int(len(set(q_labels)))\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "metrics = cmc_map(q_labels, g_labels, Q, G, ks=(1,5,10))\n",
    "print(\"[INFO] Metrics:\", metrics)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"[INFO] Saved -> {os.path.join(OUTPUT_DIR, 'metrics.json')}\")\n",
    "\n",
    "# ------------ t-SNE (subset for speed) ------------\n",
    "try:\n",
    "    subset = min(2000, len(gallery))\n",
    "    sel = np.random.choice(len(gallery), subset, replace=False)\n",
    "    X = G[sel]\n",
    "    y = np.array([g_labels[i] for i in sel])\n",
    "    print(\"[INFO] Running t-SNE on\", X.shape)\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca', random_state=SEED)\n",
    "    Z = tsne.fit_transform(X)\n",
    "    # simple scatter with tiny dots (no legend to avoid clutter)\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.scatter(Z[:,0], Z[:,1], s=4, c=np.random.RandomState(SEED).rand(len(Z)), alpha=0.7)\n",
    "    plt.title(\"t-SNE of Re-ID Embeddings (gallery subset)\")\n",
    "    out_tsne = os.path.join(OUTPUT_DIR, \"tsne.png\")\n",
    "    plt.tight_layout(); plt.savefig(out_tsne, dpi=160); plt.close()\n",
    "    print(f\"[INFO] Saved -> {out_tsne}\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] t-SNE failed:\", e)\n",
    "\n",
    "# ------------ Retrieval panels (top-5) ------------\n",
    "def load_thumb(path, side=224):\n",
    "    im = Image.open(path).convert(\"RGB\")\n",
    "    im = im.resize((side, side), Image.BILINEAR)\n",
    "    return im\n",
    "\n",
    "def draw_panel(query_path, q_label, top_paths, top_labels, top_sims, save_path):\n",
    "    pad = 8\n",
    "    cell = 200\n",
    "    cols = 6  # 1 query + 5 results\n",
    "    W = cols*cell + (cols+1)*pad\n",
    "    H = cell + 2*pad + 40\n",
    "    canvas = Image.new(\"RGB\", (W, H), (250,250,250))\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "    # fonts (fallback to default if not available)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # place query\n",
    "    q = load_thumb(query_path, side=cell)\n",
    "    canvas.paste(q, (pad, pad))\n",
    "    draw.text((pad, pad+cell+5), f\"QUERY\\n{q_label}\", fill=(0,0,0), font=font)\n",
    "\n",
    "    # place results\n",
    "    for i,(p,l,s) in enumerate(zip(top_paths, top_labels, top_sims), start=1):\n",
    "        x = i*(cell+pad) + pad\n",
    "        y = pad\n",
    "        im = load_thumb(p, side=cell)\n",
    "        canvas.paste(im, (x, y))\n",
    "        col = (0,140,0) if l == q_label else (180,0,0)\n",
    "        txt = f\"{l}  sim={s:.3f}\"\n",
    "        draw.text((x, y+cell+5), txt, fill=col, font=font)\n",
    "\n",
    "    canvas.save(save_path)\n",
    "\n",
    "N_EXAMPLES = min(6, len(queries))\n",
    "example_q = np.random.choice(len(queries), N_EXAMPLES, replace=False)\n",
    "for idx in example_q:\n",
    "    q = Q[idx]\n",
    "    qlab = q_labels[idx]\n",
    "    order, sims = rank_search(q, G)\n",
    "    topk = 5\n",
    "    inds = order[:topk]\n",
    "    tops = [gallery[i] for i in inds]\n",
    "    tlabs = [g_labels[i] for i in inds]\n",
    "    tsims = sims[:topk]\n",
    "    out = os.path.join(OUTPUT_DIR, f\"retrieval_example_{idx}.png\")\n",
    "    draw_panel(queries[idx], qlab, tops, tlabs, tsims, out)\n",
    "    print(f\"[INFO] Saved -> {out}\")\n",
    "\n",
    "# ------------ Grad-CAM on a few queries (if conv exists) ------------\n",
    "def find_last_conv(model):\n",
    "    for l in reversed(model.layers):\n",
    "        if isinstance(l, (layers.Conv2D, layers.SeparableConv2D, layers.DepthwiseConv2D)):\n",
    "            return l.name\n",
    "    return None\n",
    "\n",
    "last_conv_name = find_last_conv(model)\n",
    "if last_conv_name is None:\n",
    "    print(\"[WARN] No conv layer found; skipping Grad-CAM.\")\n",
    "else:\n",
    "    try:\n",
    "        grad_model = keras.Model(\n",
    "            [model.inputs],\n",
    "            [model.get_layer(last_conv_name).output, model.output]\n",
    "        )\n",
    "        def gradcam(path, save_path):\n",
    "            img = load_and_preprocess(path)[None, ...]\n",
    "            with tf.GradientTape() as tape:\n",
    "                conv_out, preds = grad_model(img, training=False)\n",
    "                class_idx = tf.argmax(preds[0])\n",
    "                loss = preds[:, class_idx]\n",
    "            grads = tape.gradient(loss, conv_out)\n",
    "            pooled = tf.reduce_mean(grads, axis=(1,2), keepdims=True)\n",
    "            cam = tf.nn.relu(tf.reduce_sum(pooled * conv_out, axis=-1))[0]  # (H,W)\n",
    "            cam = cam.numpy()\n",
    "            cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "            # overlay\n",
    "            base = Image.open(path).convert(\"RGB\").resize((IMG_SIZE[1], IMG_SIZE[0]))\n",
    "            heat = Image.fromarray(np.uint8(255*cam)).resize(base.size, Image.BILINEAR)\n",
    "            heat = heat.convert(\"RGBA\")\n",
    "            # colorize heatmap (simple red colormap)\n",
    "            r = np.array(heat)\n",
    "            rgba = np.zeros((r.shape[0], r.shape[1], 4), dtype=np.uint8)\n",
    "            rgba[...,0] = r   # R\n",
    "            rgba[...,3] = (r*0.6).astype(np.uint8)  # alpha\n",
    "            heat_col = Image.fromarray(rgba, mode=\"RGBA\")\n",
    "            out = base.copy()\n",
    "            out.paste(heat_col, (0,0), heat_col)\n",
    "            out.save(save_path)\n",
    "\n",
    "        N_GC = min(4, len(queries))\n",
    "        picks = np.random.choice(len(queries), N_GC, replace=False)\n",
    "        for idx in picks:\n",
    "            out = os.path.join(OUTPUT_DIR, f\"gradcam_{idx}.png\")\n",
    "            gradcam(queries[idx], out)\n",
    "            print(f\"[INFO] Saved -> {out}\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Grad-CAM failed:\", e)\n",
    "\n",
    "print(\"\\n[INFO] Done. Artifacts:\")\n",
    "print(\" - metrics.json\")\n",
    "print(\" - tsne.png\")\n",
    "print(\" - retrieval_example_*.png\")\n",
    "print(\" - gradcam_*.png (if available)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03104726-fd5a-4834-a7df-872a299aa106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
